"""
æ¨¡å‹å¯¹æ¯”ä¸å¯è§†åŒ–è„šæœ¬
==================
å¯¹æ¯”æ‰€æœ‰è®­ç»ƒçš„æ¨¡å‹å¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
"""
import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
import json
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from src.utils import setup_logger, log_section

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")


class ModelComparator:
    """æ¨¡å‹å¯¹æ¯”å™¨"""
    
    def __init__(self, logger):
        self.logger = logger
        self.ml_dir = Path('results/stage1_1d/traditional_ml')
        self.nn_dir = Path('results/stage1_1d/neural_network')
        self.output_dir = Path('results/stage1_1d/comparison')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_results(self):
        """åŠ è½½æ‰€æœ‰ç»“æœ"""
        log_section(self.logger, "åŠ è½½æ¨¡å‹ç»“æœ")
        
        results = {}
        
        # åŠ è½½ä¼ ç»ŸMLç»“æœ
        ml_results_path = self.ml_dir / 'results_summary.json'
        if ml_results_path.exists():
            with open(ml_results_path, 'r') as f:
                ml_results = json.load(f)
                results.update(ml_results)
                self.logger.info(f"åŠ è½½äº† {len(ml_results)} ä¸ªä¼ ç»ŸMLæ¨¡å‹")
        
        # åŠ è½½DNNç»“æœ
        nn_results_path = self.nn_dir / 'results.json'
        if nn_results_path.exists():
            with open(nn_results_path, 'r') as f:
                nn_results = json.load(f)
                results['DNN'] = nn_results
                self.logger.info("åŠ è½½äº† DNN æ¨¡å‹")
        
        self.logger.info(f"\næ€»å…±åŠ è½½äº† {len(results)} ä¸ªæ¨¡å‹")
        
        return results
    
    def create_comparison_table(self, results):
        """åˆ›å»ºè¯¦ç»†å¯¹æ¯”è¡¨"""
        log_section(self.logger, "ç”Ÿæˆå¯¹æ¯”è¡¨")
        
        # å‡†å¤‡æ•°æ®
        rows = []
        for model_name, model_results in results.items():
            test_metrics = model_results['test_metrics']
            rows.append({
                'Model': model_name,
                'Accuracy': test_metrics['accuracy'],
                'Precision': test_metrics['precision'],
                'Recall': test_metrics['recall'],
                'F1': test_metrics['f1'],
                'ROC-AUC': test_metrics['roc_auc'],
                'PR-AUC': test_metrics['pr_auc']
            })
        
        df = pd.DataFrame(rows)
        df = df.sort_values('ROC-AUC', ascending=False)
        
        # ä¿å­˜CSV
        csv_path = self.output_dir / 'comparison_table.csv'
        df.to_csv(csv_path, index=False)
        
        # ä¿å­˜æ ¼å¼åŒ–è¡¨æ ¼
        table_path = self.output_dir / 'comparison_table.txt'
        with open(table_path, 'w', encoding='utf-8') as f:
            f.write("="*90 + "\n")
            f.write("æ¨¡å‹æ€§èƒ½å¯¹æ¯” (æµ‹è¯•é›†)\n")
            f.write("="*90 + "\n\n")
            f.write(df.to_string(index=False))
            f.write("\n\n" + "="*90 + "\n")
            
            # æœ€ä½³æ¨¡å‹
            best_model = df.iloc[0]
            f.write(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['Model']}\n")
            f.write(f"   ROC-AUC: {best_model['ROC-AUC']:.4f}\n")
            f.write(f"   F1 Score: {best_model['F1']:.4f}\n")
        
        self.logger.info(f"å¯¹æ¯”è¡¨å·²ä¿å­˜: {table_path}")
        
        return df
    
    def plot_metrics_comparison(self, df):
        """ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾"""
        self.logger.info("\nç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾...")
        
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC', 'PR-AUC']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            
            # æ’åº
            df_sorted = df.sort_values(metric, ascending=True)
            
            # ç»˜åˆ¶æ¡å½¢å›¾
            bars = ax.barh(df_sorted['Model'], df_sorted[metric])
            
            # é¢œè‰²
            colors = plt.cm.RdYlGn(df_sorted[metric])
            for bar, color in zip(bars, colors):
                bar.set_color(color)
            
            ax.set_xlabel(metric, fontsize=12)
            ax.set_xlim(0, 1)
            ax.grid(axis='x', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (model, value) in enumerate(zip(df_sorted['Model'], df_sorted[metric])):
                ax.text(value + 0.01, i, f'{value:.3f}', va='center', fontsize=10)
        
        plt.tight_layout()
        plot_path = self.output_dir / 'metrics_comparison.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"  ä¿å­˜: {plot_path}")
    
    def plot_roc_auc_comparison(self, df):
        """ç»˜åˆ¶ROC-AUCæ’åå›¾"""
        self.logger.info("ç»˜åˆ¶ROC-AUCæ’åå›¾...")
        
        plt.figure(figsize=(10, 6))
        
        df_sorted = df.sort_values('ROC-AUC', ascending=True)
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(df_sorted)))
        bars = plt.barh(df_sorted['Model'], df_sorted['ROC-AUC'], color=colors)
        
        plt.xlabel('ROC-AUC Score', fontsize=14, fontweight='bold')
        plt.title('æ¨¡å‹ROC-AUCæ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.xlim(0.5, 1.0)
        plt.grid(axis='x', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼
        for i, (model, value) in enumerate(zip(df_sorted['Model'], df_sorted['ROC-AUC'])):
            plt.text(value + 0.01, i, f'{value:.4f}', va='center', fontweight='bold')
        
        plt.tight_layout()
        plot_path = self.output_dir / 'roc_auc_ranking.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"  ä¿å­˜: {plot_path}")
    
    def plot_radar_chart(self, df):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        self.logger.info("ç»˜åˆ¶é›·è¾¾å›¾...")
        
        # é€‰æ‹©å‰5ä¸ªæ¨¡å‹
        top_models = df.nlargest(5, 'ROC-AUC')
        
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        for idx, (_, row) in enumerate(top_models.iterrows()):
            values = [row[m] for m in metrics]
            values += values[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'])
            ax.fill(angles, values, alpha=0.15)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics, size=12)
        ax.set_ylim(0, 1)
        ax.grid(True)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        
        plt.title('Top 5 æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾', size=16, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plot_path = self.output_dir / 'radar_chart.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"  ä¿å­˜: {plot_path}")
    
    def plot_heatmap(self, df):
        """ç»˜åˆ¶æ€§èƒ½çƒ­åŠ›å›¾"""
        self.logger.info("ç»˜åˆ¶æ€§èƒ½çƒ­åŠ›å›¾...")
        
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC', 'PR-AUC']
        data = df.set_index('Model')[metrics].T
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(
            data,
            annot=True,
            fmt='.3f',
            cmap='RdYlGn',
            vmin=0.5,
            vmax=1.0,
            cbar_kws={'label': 'Score'},
            linewidths=0.5
        )
        
        plt.title('æ¨¡å‹æ€§èƒ½çƒ­åŠ›å›¾', fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('Model', fontsize=14, fontweight='bold')
        plt.ylabel('Metric', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plot_path = self.output_dir / 'performance_heatmap.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"  ä¿å­˜: {plot_path}")
    
    def generate_summary_report(self, df, results):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        log_section(self.logger, "ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
        
        report_path = self.output_dir / 'SUMMARY_REPORT.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# NLRP3 ç­›é€‰æ¨¡å‹è®­ç»ƒæŠ¥å‘Š\n\n")
            f.write("## ğŸ“Š å®éªŒæ¦‚è§ˆ\n\n")
            f.write(f"- è®­ç»ƒæ¨¡å‹æ•°é‡: {len(results)}\n")
            f.write(f"- ç‰¹å¾ç±»å‹: ECFP4æŒ‡çº¹ (2048ä½)\n")
            f.write(f"- æ•°æ®é›†åˆ’åˆ†: 70% è®­ç»ƒ / 15% éªŒè¯ / 15% æµ‹è¯•\n\n")
            
            f.write("## ğŸ† æ¨¡å‹æ’å (æŒ‰ROC-AUC)\n\n")
            f.write("| æ’å | æ¨¡å‹ | ROC-AUC | F1 Score | Precision | Recall |\n")
            f.write("|------|------|---------|----------|-----------|--------|\n")
            
            for idx, (_, row) in enumerate(df.iterrows(), 1):
                f.write(
                    f"| {idx} | {row['Model']} | {row['ROC-AUC']:.4f} | "
                    f"{row['F1']:.4f} | {row['Precision']:.4f} | {row['Recall']:.4f} |\n"
                )
            
            f.write("\n## ğŸ“ˆ æœ€ä½³æ¨¡å‹\n\n")
            best = df.iloc[0]
            f.write(f"**{best['Model']}** åœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼š\n\n")
            f.write(f"- ROC-AUC: {best['ROC-AUC']:.4f}\n")
            f.write(f"- F1 Score: {best['F1']:.4f}\n")
            f.write(f"- Precision: {best['Precision']:.4f}\n")
            f.write(f"- Recall: {best['Recall']:.4f}\n\n")
            
            f.write("## ğŸ“Š å¯è§†åŒ–\n\n")
            f.write("ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶ï¼š\n")
            f.write("- `metrics_comparison.png` - å…¨æŒ‡æ ‡å¯¹æ¯”\n")
            f.write("- `roc_auc_ranking.png` - ROC-AUCæ’å\n")
            f.write("- `radar_chart.png` - Top 5æ¨¡å‹é›·è¾¾å›¾\n")
            f.write("- `performance_heatmap.png` - æ€§èƒ½çƒ­åŠ›å›¾\n\n")
            
            f.write("## ğŸ’¡ ç»“è®º\n\n")
            f.write(f"ç¬¬ä¸€é˜¶æ®µï¼ˆ1Dç‰¹å¾ï¼‰è®­ç»ƒå®Œæˆï¼Œå…±è®­ç»ƒ{len(results)}ä¸ªæ¨¡å‹ã€‚\n")
            f.write(f"æ¨èä½¿ç”¨ **{best['Model']}** è¿›è¡Œè™šæ‹Ÿç­›é€‰ã€‚\n\n")
            
            f.write("## ğŸ“ æ–‡ä»¶ä½ç½®\n\n")
            f.write("```\n")
            f.write("results/stage1_1d/\n")
            f.write("â”œâ”€â”€ traditional_ml/       # ä¼ ç»ŸMLæ¨¡å‹\n")
            f.write("â”œâ”€â”€ neural_network/       # DNNæ¨¡å‹\n")
            f.write("â””â”€â”€ comparison/           # å¯¹æ¯”ç»“æœå’Œå¯è§†åŒ–\n")
            f.write("```\n")
        
        self.logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´å¯¹æ¯”æµç¨‹"""
        log_section(self.logger, "æ¨¡å‹å¯¹æ¯”ä¸å¯è§†åŒ–")
        
        # 1. åŠ è½½ç»“æœ
        results = self.load_results()
        
        if len(results) == 0:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ç»“æœï¼")
            return
        
        # 2. åˆ›å»ºå¯¹æ¯”è¡¨
        df = self.create_comparison_table(results)
        
        # 3. ç”Ÿæˆå¯è§†åŒ–
        log_section(self.logger, "ç”Ÿæˆå¯è§†åŒ–")
        self.plot_metrics_comparison(df)
        self.plot_roc_auc_comparison(df)
        self.plot_radar_chart(df)
        self.plot_heatmap(df)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        self.generate_summary_report(df, results)
        
        log_section(self.logger, "å¯¹æ¯”å®Œæˆ")
        
        return df


def main():
    """ä¸»å‡½æ•°"""
    logger = setup_logger("Model_Comparator")
    
    comparator = ModelComparator(logger)
    
    try:
        df = comparator.run()
        
        print(f"\n{'='*70}")
        print("[OK] æ¨¡å‹å¯¹æ¯”å®Œæˆ")
        print(f"{'='*70}")
        
        print(f"\n[BEST] æœ€ä½³æ¨¡å‹:")
        best = df.iloc[0]
        print(f"  {best['Model']}")
        print(f"  ROC-AUC: {best['ROC-AUC']:.4f}")
        print(f"  F1 Score: {best['F1']:.4f}")
        
        print(f"\nğŸ“Š æŸ¥çœ‹ç»“æœ:")
        print(f"  æŠ¥å‘Š: results/stage1_1d/comparison/SUMMARY_REPORT.md")
        print(f"  å¯è§†åŒ–: results/stage1_1d/comparison/*.png")
        
        print(f"\n[DONE] ç¬¬ä¸€é˜¶æ®µï¼ˆ1Dç‰¹å¾ï¼‰è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"\n[ERROR] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
