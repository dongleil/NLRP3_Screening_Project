"""
æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†è„šæœ¬
- æ›´åˆç†çš„æ´»æ€§é˜ˆå€¼
- æ›´å°‘çš„æ•°æ®æµå¤±
- æ›´å¹³è¡¡çš„æ•°æ®é›†
"""
import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, Dict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from src.utils import (
    setup_logger, load_data_config, log_section, log_dict,
    MoleculeProcessor, MoleculeValidator, get_inchi_key,
    calculate_descriptors
)


class ImprovedDataPreprocessor:
    """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, config: dict):
        self.config = config
        self.logger = setup_logger("Improved_Preprocessor")
        self.mol_processor = MoleculeProcessor()
        self.mol_validator = MoleculeValidator(
            mw_range=tuple(config['filtering']['molecular_weight_range']),
            heavy_atom_range=tuple(config['filtering']['heavy_atom_count_range'])
        )
        
        # æ”¹è¿›çš„é˜ˆå€¼è®¾ç½®
        self.thresholds = self._get_improved_thresholds()
    
    def _get_improved_thresholds(self) -> Dict[str, Dict[str, float]]:
        """
        è·å–æ”¹è¿›çš„æ´»æ€§é˜ˆå€¼
        
        ç­–ç•¥ï¼š
        1. é™ä½æ´»æ€§é˜ˆå€¼ï¼Œæé«˜éæ´»æ€§é˜ˆå€¼
        2. ç¼©å°ä¸ç¡®å®šåŒºåŸŸ
        3. ä¸ºä¸åŒæ´»æ€§ç±»å‹è®¾ç½®ä¸åŒé˜ˆå€¼
        """
        return {
            'IC50': {
                'active': 5.0,      # <5Î¼M ä¸ºæ´»æ€§ (åŸ10Î¼M)
                'inactive': 30.0,   # >30Î¼M ä¸ºéæ´»æ€§ (åŸ50Î¼M)
                'weight': 1.0
            },
            'EC50': {
                'active': 5.0,
                'inactive': 30.0,
                'weight': 1.0
            },
            'Ki': {
                'active': 1.0,      # Kié€šå¸¸æ›´ä¸¥æ ¼
                'inactive': 20.0,
                'weight': 1.2
            },
            'Kd': {
                'active': 1.0,
                'inactive': 20.0,
                'weight': 1.2
            },
            'AC50': {
                'active': 5.0,
                'inactive': 30.0,
                'weight': 1.0
            }
        }
    
    def load_raw_data(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½åŸå§‹æ•°æ®"""
        self.logger.info(f"åŠ è½½åŸå§‹æ•°æ®: {file_path}")
        df = pd.read_csv(file_path)
        self.logger.info(f"  åŠ è½½äº† {len(df)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºæ•°æ®æ¥æº
        if 'data_source' in df.columns:
            self.logger.info(f"\n  æ•°æ®æ¥æºåˆ†å¸ƒ:")
            for source, count in df['data_source'].value_counts().items():
                self.logger.info(f"    {source}: {count}")
        
        return df
    
    def convert_units(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç»Ÿä¸€å•ä½è½¬æ¢ä¸º Î¼M"""
        self.logger.info("\nç»Ÿä¸€å•ä½è½¬æ¢...")
        
        df = df.copy()
        df['value_um'] = np.nan
        
        # åªå¤„ç†æœ‰æ•°å€¼ä¸”relationä¸º'='çš„æ•°æ®
        mask_valid = (
            df['standard_value'].notna() & 
            (df['standard_relation'] == '=')
        )
        
        # nM -> Î¼M
        mask_nm = mask_valid & (df['standard_units'] == 'nM')
        df.loc[mask_nm, 'value_um'] = df.loc[mask_nm, 'standard_value'] / 1000
        
        # Î¼M -> Î¼M
        mask_um = mask_valid & (df['standard_units'].isin(['uM', 'Î¼M', 'UM']))
        df.loc[mask_um, 'value_um'] = df.loc[mask_um, 'standard_value']
        
        # mM -> Î¼M
        mask_mm = mask_valid & (df['standard_units'].isin(['mM', 'MM']))
        df.loc[mask_mm, 'value_um'] = df.loc[mask_mm, 'standard_value'] * 1000
        
        # åˆ é™¤æ— æ³•è½¬æ¢çš„æ•°æ®
        df = df.dropna(subset=['value_um'])
        
        converted = len(df)
        self.logger.info(f"  âœ“ å•ä½è½¬æ¢å®Œæˆ")
        self.logger.info(f"  ä¿ç•™ {converted} æ¡æœ‰æ•ˆè®°å½•")
        
        return df
    
    def assign_labels_improved(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ”¹è¿›çš„æ ‡ç­¾åˆ†é…ç­–ç•¥
        
        æ”¹è¿›ç‚¹ï¼š
        1. æ›´åˆç†çš„é˜ˆå€¼
        2. å¯¹ä¸­é—´åŒºåŸŸä½¿ç”¨åŠ æƒç­–ç•¥
        3. ä¿ç•™æ›´å¤šæ•°æ®
        """
        self.logger.info("\næ”¹è¿›çš„æ ‡ç­¾åˆ†é…...")
        self.logger.info(f"{'='*70}")
        
        df = df.copy()
        df['label'] = -1
        df['label_confidence'] = 0.0  # æ ‡ç­¾ç½®ä¿¡åº¦
        
        # æ”¯æŒçš„æ´»æ€§ç±»å‹
        supported_types = list(self.thresholds.keys())
        self.logger.info(f"æ”¯æŒçš„æ´»æ€§ç±»å‹: {supported_types}")
        
        # æ˜¾ç¤ºé˜ˆå€¼
        self.logger.info(f"\næ´»æ€§åˆ¤æ–­æ ‡å‡†ï¼ˆæ”¹è¿›ç‰ˆï¼‰:")
        for atype, thres in self.thresholds.items():
            self.logger.info(
                f"  {atype}: æ´»æ€§<{thres['active']}Î¼M, "
                f"éæ´»æ€§>{thres['inactive']}Î¼M"
            )
        
        # ç»Ÿè®¡æ¯ç§ç±»å‹
        stats = {}
        for atype in supported_types:
            mask_type = df['standard_type'] == atype
            data_type = df[mask_type]
            
            if len(data_type) == 0:
                continue
            
            thres = self.thresholds[atype]
            
            # æ´»æ€§åŒ–åˆç‰©
            mask_active = mask_type & (df['value_um'] < thres['active'])
            df.loc[mask_active, 'label'] = 1
            df.loc[mask_active, 'label_confidence'] = 1.0
            
            # éæ´»æ€§åŒ–åˆç‰©
            mask_inactive = mask_type & (df['value_um'] > thres['inactive'])
            df.loc[mask_inactive, 'label'] = 0
            df.loc[mask_inactive, 'label_confidence'] = 1.0
            
            # ä¸­é—´åŒºåŸŸï¼ˆä¸ç¡®å®šï¼‰
            mask_uncertain = mask_type & (
                (df['value_um'] >= thres['active']) & 
                (df['value_um'] <= thres['inactive'])
            )
            
            # å¯¹ä¸­é—´åŒºåŸŸä½¿ç”¨æ¸å˜æ ‡ç­¾ï¼ˆæ¥è¿‘activeé˜ˆå€¼çš„æ ‡ä¸ºæ´»æ€§ï¼‰
            for idx in df[mask_uncertain].index:
                value = df.loc[idx, 'value_um']
                
                # çº¿æ€§æ’å€¼è®¡ç®—ç½®ä¿¡åº¦
                ratio = (value - thres['active']) / (thres['inactive'] - thres['active'])
                
                if ratio < 0.5:  # æ›´æ¥è¿‘æ´»æ€§é˜ˆå€¼
                    df.loc[idx, 'label'] = 1
                    df.loc[idx, 'label_confidence'] = 1 - ratio * 2  # 0.5-1.0
                else:  # æ›´æ¥è¿‘éæ´»æ€§é˜ˆå€¼
                    df.loc[idx, 'label'] = 0
                    df.loc[idx, 'label_confidence'] = (ratio - 0.5) * 2  # 0.0-0.5
            
            # ç»Ÿè®¡
            n_active = (df[mask_type]['label'] == 1).sum()
            n_inactive = (df[mask_type]['label'] == 0).sum()
            n_uncertain = mask_uncertain.sum()
            
            stats[atype] = {
                'total': len(data_type),
                'active': n_active,
                'inactive': n_inactive,
                'uncertain_assigned': n_uncertain
            }
        
        # æ˜¾ç¤ºç»Ÿè®¡
        self.logger.info(f"\næ ‡ç­¾åˆ†é…ç»“æœ:")
        for atype, stat in stats.items():
            self.logger.info(
                f"  {atype}: æ€»æ•°={stat['total']}, "
                f"æ´»æ€§={stat['active']}, "
                f"éæ´»æ€§={stat['inactive']}, "
                f"ä¸­é—´åŒº(å·²åˆ†é…)={stat['uncertain_assigned']}"
            )
        
        # æ€»ä½“ç»Ÿè®¡
        n_active = (df['label'] == 1).sum()
        n_inactive = (df['label'] == 0).sum()
        n_unassigned = (df['label'] == -1).sum()
        
        self.logger.info(f"\n{'='*70}")
        self.logger.info(f"æ€»ä½“ç»Ÿè®¡:")
        self.logger.info(f"  æ´»æ€§: {n_active}")
        self.logger.info(f"  éæ´»æ€§: {n_inactive}")
        self.logger.info(f"  æœªåˆ†é…: {n_unassigned}")
        self.logger.info(f"  æ´»æ€§/éæ´»æ€§æ¯”ä¾‹: {n_active/max(n_inactive,1):.2f}:1")
        self.logger.info(f"  æ•°æ®ä¿ç•™ç‡: {(n_active+n_inactive)/len(df)*100:.1f}%")
        self.logger.info(f"{'='*70}")
        
        # ç§»é™¤æœªåˆ†é…çš„ï¼ˆåº”è¯¥å¾ˆå°‘æˆ–æ²¡æœ‰ï¼‰
        if n_unassigned > 0:
            self.logger.info(f"\nç§»é™¤ {n_unassigned} æ¡æœªåˆ†é…æ•°æ®")
            df = df[df['label'] != -1]
        
        return df
    
    def standardize_molecules(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–åˆ†å­ç»“æ„"""
        self.logger.info("\næ ‡å‡†åŒ–åˆ†å­ç»“æ„...")
        
        results = []
        failed_count = 0
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æ ‡å‡†åŒ–åˆ†å­"):
            smiles = row['canonical_smiles']
            
            # å¤„ç†SMILES
            canonical_smiles, mol = self.mol_processor.process_smiles(smiles)
            
            if canonical_smiles is None:
                failed_count += 1
                continue
            
            # éªŒè¯åˆ†å­
            if not self.mol_validator.is_valid(mol):
                failed_count += 1
                continue
            
            # è·å–InChI Key
            inchi_key = get_inchi_key(mol)
            if inchi_key is None:
                failed_count += 1
                continue
            
            # è®¡ç®—æè¿°ç¬¦
            descriptors = calculate_descriptors(mol)
            
            # ä¿å­˜ç»“æœ
            result = row.to_dict()
            result['smiles_standardized'] = canonical_smiles
            result['inchi_key'] = inchi_key
            result.update(descriptors)
            
            results.append(result)
        
        df_clean = pd.DataFrame(results)
        
        self.logger.info(f"  âœ“ æ ‡å‡†åŒ–å®Œæˆ")
        self.logger.info(f"  æˆåŠŸ: {len(df_clean)}")
        self.logger.info(f"  å¤±è´¥: {failed_count}")
        self.logger.info(f"  æˆåŠŸç‡: {len(df_clean)/(len(df_clean)+failed_count)*100:.1f}%")
        
        return df_clean
    
    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """å»é‡ - æ”¹è¿›ç­–ç•¥"""
        self.logger.info("\næ™ºèƒ½å»é‡...")
        
        initial_count = len(df)
        
        # æŒ‰InChI Keyåˆ†ç»„
        duplicates = df.groupby('inchi_key').size()
        n_duplicates = (duplicates > 1).sum()
        
        self.logger.info(f"  å‘ç° {n_duplicates} ä¸ªåˆ†å­æœ‰é‡å¤è®°å½•")
        
        # å¯¹äºé‡å¤çš„åˆ†å­ï¼Œé€‰æ‹©æœ€å¯é çš„æ•°æ®
        def select_best_record(group):
            """é€‰æ‹©æœ€ä½³è®°å½•"""
            # ä¼˜å…ˆçº§ï¼š
            # 1. ç½®ä¿¡åº¦æœ€é«˜
            # 2. IC50 > Ki > EC50
            # 3. å€¼æœ€å°ï¼ˆå¯¹äºæ´»æ€§åŒ–åˆç‰©ï¼‰
            
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            priority_map = {'IC50': 3, 'Ki': 2, 'EC50': 1, 'Kd': 2, 'AC50': 1}
            group['priority'] = group['standard_type'].map(lambda x: priority_map.get(x, 0))
            
            group = group.sort_values(
                by=['label_confidence', 'priority', 'value_um'],
                ascending=[False, False, True]
            )
            
            return group.iloc[0]
        
        df_dedup = df.groupby('inchi_key', group_keys=False).apply(select_best_record)
        df_dedup = df_dedup.reset_index(drop=True)
        
        removed = initial_count - len(df_dedup)
        self.logger.info(f"  ç§»é™¤äº† {removed} ä¸ªé‡å¤è®°å½•")
        self.logger.info(f"  å‰©ä½™ {len(df_dedup)} ä¸ªå”¯ä¸€åˆ†å­")
        
        # æ˜¾ç¤ºå»é‡åçš„åˆ†å¸ƒ
        n_active = (df_dedup['label'] == 1).sum()
        n_inactive = (df_dedup['label'] == 0).sum()
        self.logger.info(f"  å»é‡å: æ´»æ€§={n_active}, éæ´»æ€§={n_inactive}")
        
        return df_dedup
    
    def balance_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®å¹³è¡¡ - æ”¹è¿›ç­–ç•¥"""
        method = self.config['balancing']['method']
        target_ratio = self.config['balancing']['target_ratio']
        
        self.logger.info(f"\næ•°æ®å¹³è¡¡ (æ–¹æ³•: {method}, ç›®æ ‡æ¯”ä¾‹: {target_ratio})")
        
        n_active = (df['label'] == 1).sum()
        n_inactive = (df['label'] == 0).sum()
        
        self.logger.info(f"  åŸå§‹åˆ†å¸ƒ: æ´»æ€§={n_active}, éæ´»æ€§={n_inactive}")
        self.logger.info(f"  åŸå§‹æ¯”ä¾‹: {n_active/max(n_inactive,1):.2f}:1")
        
        if method == "undersample":
            target_inactive = int(n_active / target_ratio)
            
            if n_inactive > target_inactive:
                df_active = df[df['label'] == 1]
                df_inactive = df[df['label'] == 0].sample(
                    n=target_inactive, random_state=42
                )
                df = pd.concat([df_active, df_inactive])
                
                self.logger.info(f"  âœ“ æ¬ é‡‡æ ·: æ´»æ€§={len(df_active)}, éæ´»æ€§={len(df_inactive)}")
            else:
                self.logger.info(f"  éæ´»æ€§æ ·æœ¬å·²å°‘äºç›®æ ‡ï¼Œä¸è¿›è¡Œæ¬ é‡‡æ ·")
        
        elif method == "oversample":
            # è¿‡é‡‡æ ·å°‘æ•°ç±»
            if n_inactive < n_active / target_ratio:
                target_inactive = int(n_active / target_ratio)
                df_active = df[df['label'] == 1]
                df_inactive = df[df['label'] == 0]
                
                # é‡å¤é‡‡æ ·
                df_inactive_over = df_inactive.sample(
                    n=target_inactive, replace=True, random_state=42
                )
                df = pd.concat([df_active, df_inactive_over])
                
                self.logger.info(f"  âœ“ è¿‡é‡‡æ ·: æ´»æ€§={len(df_active)}, éæ´»æ€§={len(df_inactive_over)}")
        
        final_active = (df['label'] == 1).sum()
        final_inactive = (df['label'] == 0).sum()
        self.logger.info(f"  æœ€ç»ˆåˆ†å¸ƒ: æ´»æ€§={final_active}, éæ´»æ€§={final_inactive}")
        self.logger.info(f"  æœ€ç»ˆæ¯”ä¾‹: {final_active/max(final_inactive,1):.2f}:1")
        
        return df.reset_index(drop=True)
    
    def save_processed_data(self, df: pd.DataFrame, output_path: str):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        columns_to_save = [
            'molecule_chembl_id',
            'smiles_standardized',
            'inchi_key',
            'standard_type',
            'value_um',
            'label',
            'label_confidence',  # æ–°å¢
            'MW', 'LogP', 'TPSA', 'HBA', 'HBD',
            'RotatableBonds', 'AromaticRings', 'HeavyAtoms'
        ]
        
        # æ£€æŸ¥å“ªäº›åˆ—å­˜åœ¨
        existing_cols = [col for col in columns_to_save if col in df.columns]
        
        df_save = df[existing_cols].copy()
        df_save.to_csv(output_path, index=False)
        
        self.logger.info(f"\næ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    def save_statistics(self, df: pd.DataFrame, output_path: str):
        """ä¿å­˜æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        import json
        
        stats = {
            "æ€»ä½“ç»Ÿè®¡": {
                "total_compounds": len(df),
                "active_compounds": int((df['label'] == 1).sum()),
                "inactive_compounds": int((df['label'] == 0).sum()),
                "active_inactive_ratio": float((df['label'] == 1).sum() / max((df['label'] == 0).sum(), 1))
            },
            "æ´»æ€§ç±»å‹åˆ†å¸ƒ": df['standard_type'].value_counts().to_dict(),
            "åˆ†å­æ€§è´¨": {
                "MW": {
                    "mean": float(df['MW'].mean()),
                    "std": float(df['MW'].std()),
                    "min": float(df['MW'].min()),
                    "max": float(df['MW'].max())
                },
                "LogP": {
                    "mean": float(df['LogP'].mean()),
                    "std": float(df['LogP'].std())
                }
            },
            "æ ‡ç­¾ç½®ä¿¡åº¦": {
                "mean": float(df['label_confidence'].mean()),
                "high_confidence": int((df['label_confidence'] > 0.8).sum()),
                "medium_confidence": int(((df['label_confidence'] > 0.5) & (df['label_confidence'] <= 0.8)).sum()),
                "low_confidence": int((df['label_confidence'] <= 0.5).sum())
            }
        }
        
        output_path = Path(output_path)
        with open(output_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_path}")
    
    def run(self, input_path: str) -> str:
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        log_section(self.logger, "æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†")
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_raw_data(input_path)
        
        # 2. å•ä½è½¬æ¢
        df = self.convert_units(df)
        
        # 3. æ”¹è¿›çš„æ ‡ç­¾åˆ†é…
        df = self.assign_labels_improved(df)
        
        # 4. æ ‡å‡†åŒ–
        df = self.standardize_molecules(df)
        
        # 5. å»é‡
        df = self.remove_duplicates(df)
        
        # 6. å¹³è¡¡
        df = self.balance_dataset(df)
        
        # 7. ä¿å­˜
        output_dir = self.config['paths']['processed_data_dir']
        output_file = self.config['filenames']['processed_data']
        output_path = Path(output_dir) / output_file
        
        self.save_processed_data(df, output_path)
        
        # 8. ç»Ÿè®¡
        stats_file = self.config['filenames']['data_statistics']
        stats_path = Path(output_dir) / stats_file
        self.save_statistics(df, stats_path)
        
        # æœ€ç»ˆæŠ¥å‘Š
        log_section(self.logger, "é¢„å¤„ç†å®Œæˆ")
        self.logger.info(f"æœ€ç»ˆæ•°æ®é›†:")
        self.logger.info(f"  æ€»åŒ–åˆç‰©æ•°: {len(df)}")
        self.logger.info(f"  æ´»æ€§åŒ–åˆç‰©: {(df['label'] == 1).sum()}")
        self.logger.info(f"  éæ´»æ€§åŒ–åˆç‰©: {(df['label'] == 0).sum()}")
        self.logger.info(f"  æ¯”ä¾‹: {(df['label'] == 1).sum() / max((df['label'] == 0).sum(), 1):.2f}:1")
        
        return str(output_path)


def main():
    """ä¸»å‡½æ•°"""
    config = load_data_config()
    
    input_dir = config['paths']['raw_data_dir']
    input_file = config['filenames']['raw_data']
    input_path = Path(input_dir) / input_file
    
    if not input_path.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°åŸå§‹æ•°æ®æ–‡ä»¶: {input_path}")
        print("è¯·å…ˆè¿è¡Œ: python experiments/stage0_data/01_download_chembl_improved.py")
        return
    
    preprocessor = ImprovedDataPreprocessor(config)
    output_path = preprocessor.run(str(input_path))
    
    print(f"\n{'='*70}")
    print("âœ“ é¢„å¤„ç†å®Œæˆ")
    print(f"{'='*70}")
    print(f"\næ–‡ä»¶ä½ç½®: {output_path}")
    print("\nğŸ’¡ æ”¹è¿›ç‚¹:")
    print("  âœ“ æ›´åˆç†çš„æ´»æ€§é˜ˆå€¼ï¼ˆæ´»æ€§<5Î¼M, éæ´»æ€§>30Î¼Mï¼‰")
    print("  âœ“ ä¸­é—´åŒºåŸŸæ™ºèƒ½åˆ†é…ï¼ˆå‡å°‘æ•°æ®æµå¤±ï¼‰")
    print("  âœ“ æ ‡ç­¾ç½®ä¿¡åº¦è¯„åˆ†")
    print("  âœ“ æ™ºèƒ½å»é‡ï¼ˆä¿ç•™æœ€å¯é è®°å½•ï¼‰")
    print("\nä¸‹ä¸€æ­¥:")
    print("  python experiments/stage0_data/03_split_dataset.py")


if __name__ == "__main__":
    main()
