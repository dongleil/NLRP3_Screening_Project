"""
NLRP3æ•°æ®é¢„å¤„ç†è„šæœ¬ - ä¸¥æ ¼ç›®æ ‡ç‰ˆ
=====================================
ç›®æ ‡ï¼š
- æ´»æ€§ï¼š900ä¸ªï¼ˆIC50<10Î¼M æˆ– EC50<10Î¼Mï¼‰
- éæ´»æ€§ï¼š2700ä¸ªï¼ˆIC50>50Î¼Mï¼‰
- æ¯”ä¾‹ï¼šæ´»æ€§:éæ´»æ€§ = 1:3

ç‰¹ç‚¹ï¼š
1. ä¸¥æ ¼çš„é˜ˆå€¼æ ‡å‡†ï¼ˆIC50<10Î¼M, EC50<10Î¼Mï¼‰
2. é›¶å®¹å¿çš„æ•°æ®ä¸¢å¼ƒ
3. ç²¾ç¡®çš„æ¯”ä¾‹æ§åˆ¶
4. å®Œæ•´çš„æ•°æ®æº¯æº
"""
import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from src.utils import (
    setup_logger, load_data_config, log_section,
    MoleculeProcessor, MoleculeValidator, get_inchi_key,
    calculate_descriptors
)


class StrictPreprocessor:
    """ä¸¥æ ¼é¢„å¤„ç†å™¨ - ç²¾ç¡®è¾¾åˆ°ç›®æ ‡"""
    
    def __init__(self, config: dict):
        self.config = config
        self.logger = setup_logger("Strict_Preprocessor")
        
        # ä¸¥æ ¼ç›®æ ‡
        self.TARGET_ACTIVE = 900
        self.TARGET_INACTIVE = 2700
        self.TARGET_RATIO = 3.0
        
        # ä¸¥æ ¼é˜ˆå€¼ï¼ˆÎ¼Mï¼‰
        self.THRESHOLDS = {
            'IC50': {'active': 10.0, 'inactive': 50.0},
            'EC50': {'active': 10.0, 'inactive': 50.0},
            'Ki': {'active': 10.0, 'inactive': 50.0},
            'Kd': {'active': 10.0, 'inactive': 50.0},
        }
        
        self.logger.info("="*70)
        self.logger.info("ä¸¥æ ¼é¢„å¤„ç†ç›®æ ‡")
        self.logger.info("="*70)
        self.logger.info(f"æ´»æ€§ç›®æ ‡: {self.TARGET_ACTIVE}")
        self.logger.info(f"éæ´»æ€§ç›®æ ‡: {self.TARGET_INACTIVE}")
        self.logger.info(f"ç›®æ ‡æ¯”ä¾‹: 1:{self.TARGET_RATIO}")
        self.logger.info(f"\nä¸¥æ ¼é˜ˆå€¼:")
        for atype, thres in self.THRESHOLDS.items():
            self.logger.info(
                f"  {atype}: æ´»æ€§<{thres['active']}Î¼M, "
                f"éæ´»æ€§>{thres['inactive']}Î¼M"
            )
        self.logger.info("="*70)
        
        self.mol_processor = MoleculeProcessor()
        self.mol_validator = MoleculeValidator(
            mw_range=tuple(config['filtering']['molecular_weight_range']),
            heavy_atom_range=tuple(config['filtering']['heavy_atom_count_range'])
        )
    
    def load_raw_data(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½åŸå§‹æ•°æ®"""
        log_section(self.logger, "åŠ è½½åŸå§‹æ•°æ®")
        
        df = pd.read_csv(file_path)
        
        self.logger.info(f"æ–‡ä»¶: {file_path}")
        self.logger.info(f"æ€»è®°å½•: {len(df)}")
        
        if 'data_source' in df.columns:
            self.logger.info(f"\næ•°æ®æ¥æº:")
            for source, count in df['data_source'].value_counts().items():
                pct = count / len(df) * 100
                self.logger.info(f"  {source:25s}: {count:5d} ({pct:5.1f}%)")
        
        # æ´»æ€§ç±»å‹ç»Ÿè®¡
        if 'standard_type' in df.columns:
            self.logger.info(f"\næ´»æ€§ç±»å‹:")
            for atype, count in df['standard_type'].value_counts().head(10).items():
                self.logger.info(f"  {atype:15s}: {count:5d}")
        
        return df
    
    def convert_units_strict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸¥æ ¼å•ä½è½¬æ¢
        
        åªä¿ç•™å¯ä»¥ç²¾ç¡®è½¬æ¢ä¸ºÎ¼Mçš„æ•°æ®
        """
        log_section(self.logger, "å•ä½è½¬æ¢")
        
        df = df.copy()
        df['value_um'] = np.nan
        
        # åªå¤„ç†æœ‰æ ‡å‡†å€¼çš„æ•°æ®
        mask_has_value = df['standard_value'].notna()
        
        self.logger.info(f"æœ‰æ ‡å‡†å€¼çš„è®°å½•: {mask_has_value.sum()}")
        
        # nM -> Î¼M (æœ€å¸¸è§)
        mask_nm = mask_has_value & (df['standard_units'] == 'nM')
        df.loc[mask_nm, 'value_um'] = df.loc[mask_nm, 'standard_value'] / 1000.0
        self.logger.info(f"  nMè½¬æ¢: {mask_nm.sum()}")
        
        # Î¼M -> Î¼M
        mask_um = mask_has_value & df['standard_units'].isin(['uM', 'Î¼M', 'UM'])
        df.loc[mask_um, 'value_um'] = df.loc[mask_um, 'standard_value']
        self.logger.info(f"  Î¼Mè½¬æ¢: {mask_um.sum()}")
        
        # mM -> Î¼M (è¾ƒå°‘)
        mask_mm = mask_has_value & df['standard_units'].isin(['mM', 'MM'])
        df.loc[mask_mm, 'value_um'] = df.loc[mask_mm, 'standard_value'] * 1000.0
        self.logger.info(f"  mMè½¬æ¢: {mask_mm.sum()}")
        
        # ç»Ÿè®¡
        initial = len(df)
        df = df.dropna(subset=['value_um'])
        final = len(df)
        removed = initial - final
        
        self.logger.info(f"\nè½¬æ¢ç»“æœ:")
        self.logger.info(f"  åˆå§‹: {initial}")
        self.logger.info(f"  æˆåŠŸ: {final} ({final/initial*100:.1f}%)")
        self.logger.info(f"  å¤±è´¥: {removed}")
        
        if final > 0:
            self.logger.info(f"\næ•°å€¼èŒƒå›´: {df['value_um'].min():.6f} - {df['value_um'].max():.2f} Î¼M")
        
        return df
    
    def assign_labels_strict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸¥æ ¼æ ‡ç­¾åˆ†é…
        
        è§„åˆ™ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1. relation='<' AND valueâ‰¤10Î¼M â†’ æ´»æ€§
        2. relation='>' â†’ éæ´»æ€§
        3. IC50/EC50/Ki/Kd='=' AND value<10Î¼M â†’ æ´»æ€§
        4. IC50/EC50/Ki/Kd='=' AND value>50Î¼M â†’ éæ´»æ€§
        5. å…¶ä»–ï¼ˆ10-50Î¼Mä¸­é—´åŒºï¼‰ â†’ æ ¹æ®æ¥æºåˆ¤æ–­
        """
        log_section(self.logger, "ä¸¥æ ¼æ ‡ç­¾åˆ†é…")
        
        df = df.copy()
        df['label'] = -1  # æœªåˆ†é…
        df['label_rule'] = ''  # æ ‡æ³¨è§„åˆ™
        df['label_confidence'] = 0.0  # ç½®ä¿¡åº¦
        
        total = len(df)
        
        # è§„åˆ™1: relation='<' ä¸” valueâ‰¤10Î¼M â†’ æ˜ç¡®æ´»æ€§
        mask_r1 = (df['standard_relation'] == '<') & (df['value_um'] <= 10.0)
        df.loc[mask_r1, 'label'] = 1
        df.loc[mask_r1, 'label_rule'] = 'relation_less'
        df.loc[mask_r1, 'label_confidence'] = 1.0
        n_r1 = mask_r1.sum()
        
        # è§„åˆ™2: relation='>' â†’ æ˜ç¡®éæ´»æ€§
        mask_r2 = df['standard_relation'] == '>'
        df.loc[mask_r2, 'label'] = 0
        df.loc[mask_r2, 'label_rule'] = 'relation_greater'
        df.loc[mask_r2, 'label_confidence'] = 1.0
        n_r2 = mask_r2.sum()
        
        # è§„åˆ™3: relation='=' ä¸” value<10Î¼M â†’ æ´»æ€§
        mask_r3 = (
            (df['standard_relation'] == '=') &
            df['standard_type'].isin(self.THRESHOLDS.keys()) &
            (df['value_um'] < 10.0)
        )
        df.loc[mask_r3, 'label'] = 1
        df.loc[mask_r3, 'label_rule'] = 'value_lt_10um'
        df.loc[mask_r3, 'label_confidence'] = 1.0
        n_r3 = mask_r3.sum()
        
        # è§„åˆ™4: relation='=' ä¸” value>50Î¼M â†’ éæ´»æ€§
        mask_r4 = (
            (df['standard_relation'] == '=') &
            df['standard_type'].isin(self.THRESHOLDS.keys()) &
            (df['value_um'] > 50.0)
        )
        df.loc[mask_r4, 'label'] = 0
        df.loc[mask_r4, 'label_rule'] = 'value_gt_50um'
        df.loc[mask_r4, 'label_confidence'] = 1.0
        n_r4 = mask_r4.sum()
        
        # è§„åˆ™5: ä¸­é—´åŒºï¼ˆ10-50Î¼Mï¼‰- æ ¹æ®æ•°æ®æ¥æºåˆ¤æ–­
        mask_middle = (
            (df['standard_relation'] == '=') &
            df['standard_type'].isin(self.THRESHOLDS.keys()) &
            (df['value_um'] >= 10.0) &
            (df['value_um'] <= 50.0)
        )
        n_middle = mask_middle.sum()
        
        # ä¸­é—´åŒºç­–ç•¥ï¼šæ ¹æ®æ¥æº
        # - NLRP3æ•°æ®ï¼šæ¥è¿‘10Î¼Mæ ‡ä¸ºæ´»æ€§ï¼Œæ¥è¿‘50Î¼Mæ ‡ä¸ºéæ´»æ€§
        # - é‡‡æ ·æ•°æ®ï¼šå…¨éƒ¨æ ‡ä¸ºéæ´»æ€§ï¼ˆä¿å®ˆï¼‰
        if n_middle > 0:
            self.logger.info(f"\nå¤„ç†ä¸­é—´åŒº (10-50Î¼M): {n_middle}æ¡")
            
            for idx in df[mask_middle].index:
                value = df.loc[idx, 'value_um']
                source = df.loc[idx, 'data_source']
                
                if 'NLRP3' in source:
                    # NLRP3æ•°æ®ï¼šçº¿æ€§æ’å€¼
                    if value < 25.0:  # æ›´æ¥è¿‘10Î¼M
                        df.loc[idx, 'label'] = 1
                        df.loc[idx, 'label_rule'] = 'middle_nlrp3_active'
                        df.loc[idx, 'label_confidence'] = 1 - (value - 10) / 15  # 0.67-1.0
                    else:  # æ›´æ¥è¿‘50Î¼M
                        df.loc[idx, 'label'] = 0
                        df.loc[idx, 'label_rule'] = 'middle_nlrp3_inactive'
                        df.loc[idx, 'label_confidence'] = (value - 25) / 25  # 0.0-1.0
                else:
                    # éNLRP3æ•°æ®ï¼šä¿å®ˆæ ‡ä¸ºéæ´»æ€§
                    df.loc[idx, 'label'] = 0
                    df.loc[idx, 'label_rule'] = 'middle_sampled_inactive'
                    df.loc[idx, 'label_confidence'] = 0.8
            
            n_middle_active = df[mask_middle & (df['label'] == 1)].shape[0]
            n_middle_inactive = df[mask_middle & (df['label'] == 0)].shape[0]
            
            self.logger.info(f"  ä¸­é—´åŒºâ†’æ´»æ€§: {n_middle_active}")
            self.logger.info(f"  ä¸­é—´åŒºâ†’éæ´»æ€§: {n_middle_inactive}")
        
        # ç»Ÿè®¡
        n_active = (df['label'] == 1).sum()
        n_inactive = (df['label'] == 0).sum()
        n_unassigned = (df['label'] == -1).sum()
        
        self.logger.info(f"\n{'='*70}")
        self.logger.info("æ ‡ç­¾åˆ†é…ç»“æœ")
        self.logger.info(f"{'='*70}")
        self.logger.info(f"è§„åˆ™1 (relation='<'):           {n_r1:5d} â†’ æ´»æ€§")
        self.logger.info(f"è§„åˆ™2 (relation='>'):           {n_r2:5d} â†’ éæ´»æ€§")
        self.logger.info(f"è§„åˆ™3 (value<10Î¼M):            {n_r3:5d} â†’ æ´»æ€§")
        self.logger.info(f"è§„åˆ™4 (value>50Î¼M):            {n_r4:5d} â†’ éæ´»æ€§")
        self.logger.info(f"è§„åˆ™5 (ä¸­é—´åŒº):                {n_middle:5d}")
        self.logger.info(f"\n{'='*70}")
        self.logger.info(f"æ´»æ€§:     {n_active:5d} ({n_active/total*100:5.1f}%)")
        self.logger.info(f"éæ´»æ€§:   {n_inactive:5d} ({n_inactive/total*100:5.1f}%)")
        self.logger.info(f"æœªåˆ†é…:   {n_unassigned:5d} ({n_unassigned/total*100:5.1f}%)")
        
        if n_active > 0 and n_inactive > 0:
            actual_ratio = n_inactive / n_active
            self.logger.info(f"\nå½“å‰æ¯”ä¾‹: 1:{actual_ratio:.2f}")
            self.logger.info(f"ç›®æ ‡æ¯”ä¾‹: 1:{self.TARGET_RATIO:.2f}")
        
        # ä¿ç•™ç‡
        retention_rate = (n_active + n_inactive) / total * 100
        self.logger.info(f"æ•°æ®ä¿ç•™ç‡: {retention_rate:.1f}%")
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        assigned = df[df['label'] != -1]
        if len(assigned) > 0:
            self.logger.info(f"\nç½®ä¿¡åº¦ç»Ÿè®¡:")
            self.logger.info(f"  å¹³å‡: {assigned['label_confidence'].mean():.3f}")
            self.logger.info(f"  é«˜(>0.9): {(assigned['label_confidence'] > 0.9).sum()}")
            self.logger.info(f"  ä¸­(0.7-0.9): {((assigned['label_confidence'] >= 0.7) & (assigned['label_confidence'] <= 0.9)).sum()}")
            self.logger.info(f"  ä½(<0.7): {(assigned['label_confidence'] < 0.7).sum()}")
        
        # å¤„ç†æœªåˆ†é…æ•°æ®
        if n_unassigned > 0:
            self.logger.warning(f"\nâš ï¸  å‘ç°{n_unassigned}æ¡æœªåˆ†é…æ•°æ®")
            
            # ä¿å­˜æœªåˆ†é…æ•°æ®ä¾›æ£€æŸ¥
            unassigned = df[df['label'] == -1].copy()
            review_path = Path(self.config['paths']['processed_data_dir']) / 'unassigned_review.csv'
            review_path.parent.mkdir(parents=True, exist_ok=True)
            unassigned.to_csv(review_path, index=False)
            
            self.logger.info(f"  æœªåˆ†é…æ•°æ®å·²ä¿å­˜: {review_path}")
            self.logger.info(f"  è¿™äº›æ•°æ®å°†è¢«ç§»é™¤")
            
            # ç§»é™¤
            df = df[df['label'] != -1].copy()
        
        self.logger.info(f"{'='*70}")
        
        return df
    
    def standardize_and_filter(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–åˆ†å­å¹¶è¿‡æ»¤
        
        ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶
        """
        log_section(self.logger, "åˆ†å­æ ‡å‡†åŒ–ä¸è´¨é‡æ§åˆ¶")
        
        results = []
        failed = {'parse': 0, 'standardize': 0, 'validate': 0, 'inchi': 0}
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†åˆ†å­"):
            smiles = row['canonical_smiles']
            
            if pd.isna(smiles):
                failed['parse'] += 1
                continue
            
            # æ ‡å‡†åŒ–
            std_smiles, mol = self.mol_processor.process_smiles(smiles)
            if std_smiles is None or mol is None:
                failed['standardize'] += 1
                continue
            
            # éªŒè¯åˆ†å­å±æ€§
            if not self.mol_validator.is_valid(mol):
                failed['validate'] += 1
                continue
            
            # InChI Key
            inchi_key = get_inchi_key(mol)
            if inchi_key is None:
                failed['inchi'] += 1
                continue
            
            # è®¡ç®—æè¿°ç¬¦
            descriptors = calculate_descriptors(mol)
            
            # ä¿å­˜
            result = row.to_dict()
            result['smiles_standardized'] = std_smiles
            result['inchi_key'] = inchi_key
            result.update(descriptors)
            
            results.append(result)
        
        df_clean = pd.DataFrame(results)
        
        total_failed = sum(failed.values())
        success_rate = len(df_clean) / (len(df_clean) + total_failed) * 100
        
        self.logger.info(f"\nå¤„ç†ç»“æœ:")
        self.logger.info(f"  âœ“ æˆåŠŸ: {len(df_clean)} ({success_rate:.1f}%)")
        self.logger.info(f"  âœ— å¤±è´¥: {total_failed}")
        
        if total_failed > 0:
            self.logger.info(f"\nå¤±è´¥åŸå› :")
            for reason, count in failed.items():
                if count > 0:
                    self.logger.info(f"    {reason}: {count}")
        
        # æ ‡ç­¾åˆ†å¸ƒ
        n_active = (df_clean['label'] == 1).sum()
        n_inactive = (df_clean['label'] == 0).sum()
        
        self.logger.info(f"\næ ‡å‡†åŒ–ååˆ†å¸ƒ:")
        self.logger.info(f"  æ´»æ€§: {n_active}")
        self.logger.info(f"  éæ´»æ€§: {n_inactive}")
        self.logger.info(f"  æ¯”ä¾‹: 1:{n_inactive/max(n_active,1):.2f}")
        
        return df_clean
    
    def intelligent_deduplication(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ™ºèƒ½å»é‡
        
        ç­–ç•¥ï¼š
        - æŒ‰InChI Keyåˆ†ç»„
        - ä¼˜å…ˆä¿ç•™é«˜ç½®ä¿¡åº¦æ ‡ç­¾
        - ä¼˜å…ˆä¿ç•™æ´»æ€§å€¼æœ€å°çš„ï¼ˆå¯¹æ´»æ€§åŒ–åˆç‰©ï¼‰
        """
        log_section(self.logger, "æ™ºèƒ½å»é‡")
        
        initial = len(df)
        
        def select_best_record(group):
            """é€‰æ‹©æœ€ä½³è®°å½•"""
            if len(group) == 1:
                return group.iloc[0]
            
            # ä¼˜å…ˆçº§1: ç½®ä¿¡åº¦æœ€é«˜
            # ä¼˜å…ˆçº§2: å¯¹äºæ´»æ€§ï¼Œé€‰IC50æœ€å°çš„
            # ä¼˜å…ˆçº§3: å¯¹äºéæ´»æ€§ï¼Œé€‰ç¬¬ä¸€æ¡
            
            label = group.iloc[0]['label']
            
            if label == 1:  # æ´»æ€§
                # æŒ‰ç½®ä¿¡åº¦å’ŒIC50æ’åº
                best_idx = group.sort_values(
                    by=['label_confidence', 'value_um'],
                    ascending=[False, True]
                ).index[0]
            else:  # éæ´»æ€§
                # æŒ‰ç½®ä¿¡åº¦æ’åº
                best_idx = group.sort_values(
                    by='label_confidence',
                    ascending=False
                ).index[0]
            
            return group.loc[best_idx]
        
        df_dedup = df.groupby('inchi_key', group_keys=False).apply(
            select_best_record
        ).reset_index(drop=True)
        
        removed = initial - len(df_dedup)
        
        self.logger.info(f"å»é‡ç»“æœ:")
        self.logger.info(f"  åˆå§‹: {initial}")
        self.logger.info(f"  ç§»é™¤: {removed} ({removed/initial*100:.1f}%)")
        self.logger.info(f"  ä¿ç•™: {len(df_dedup)}")
        
        # å»é‡ååˆ†å¸ƒ
        n_active = (df_dedup['label'] == 1).sum()
        n_inactive = (df_dedup['label'] == 0).sum()
        
        self.logger.info(f"\nå»é‡ååˆ†å¸ƒ:")
        self.logger.info(f"  æ´»æ€§: {n_active}")
        self.logger.info(f"  éæ´»æ€§: {n_inactive}")
        self.logger.info(f"  æ¯”ä¾‹: 1:{n_inactive/max(n_active,1):.2f}")
        
        return df_dedup
    
    def balance_to_exact_target(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:
        """
        ç²¾ç¡®å¹³è¡¡åˆ°ç›®æ ‡
        
        ç›®æ ‡ï¼š900æ´»æ€§ + 2700éæ´»æ€§
        
        Returns:
            (balanced_df, status_dict)
        """
        log_section(self.logger, "ç²¾ç¡®å¹³è¡¡åˆ°ç›®æ ‡")
        
        n_active = (df['label'] == 1).sum()
        n_inactive = (df['label'] == 0).sum()
        
        self.logger.info(f"å½“å‰çŠ¶æ€:")
        self.logger.info(f"  æ´»æ€§: {n_active}")
        self.logger.info(f"  éæ´»æ€§: {n_inactive}")
        self.logger.info(f"  æ¯”ä¾‹: 1:{n_inactive/max(n_active,1):.2f}")
        
        self.logger.info(f"\nç›®æ ‡:")
        self.logger.info(f"  æ´»æ€§: {self.TARGET_ACTIVE}")
        self.logger.info(f"  éæ´»æ€§: {self.TARGET_INACTIVE}")
        self.logger.info(f"  æ¯”ä¾‹: 1:{self.TARGET_RATIO}")
        
        df_active = df[df['label'] == 1].copy()
        df_inactive = df[df['label'] == 0].copy()
        
        status = {
            'active_sufficient': True,
            'inactive_sufficient': True,
            'active_shortfall': 0,
            'inactive_shortfall': 0
        }
        
        # å¤„ç†æ´»æ€§æ ·æœ¬
        self.logger.info(f"\nå¤„ç†æ´»æ€§æ ·æœ¬:")
        if n_active > self.TARGET_ACTIVE:
            self.logger.info(f"  è¿‡å¤šï¼Œæ¬ é‡‡æ ·: {n_active} â†’ {self.TARGET_ACTIVE}")
            df_active = df_active.sample(n=self.TARGET_ACTIVE, random_state=42)
        elif n_active < self.TARGET_ACTIVE:
            shortfall = self.TARGET_ACTIVE - n_active
            status['active_sufficient'] = False
            status['active_shortfall'] = shortfall
            self.logger.warning(f"  âš ï¸  ä¸è¶³! ç¼ºå£: {shortfall}")
            self.logger.warning(f"  å°†ä½¿ç”¨æ‰€æœ‰{n_active}ä¸ªæ´»æ€§æ ·æœ¬")
        else:
            self.logger.info(f"  âœ“ æ°å¥½è¾¾æ ‡: {n_active}")
        
        # æ ¹æ®æœ€ç»ˆæ´»æ€§æ•°è°ƒæ•´éæ´»æ€§ç›®æ ‡
        final_active_count = len(df_active)
        adjusted_inactive_target = int(final_active_count * self.TARGET_RATIO)
        
        # å¤„ç†éæ´»æ€§æ ·æœ¬
        self.logger.info(f"\nå¤„ç†éæ´»æ€§æ ·æœ¬:")
        self.logger.info(f"  è°ƒæ•´åç›®æ ‡: {adjusted_inactive_target}")
        
        if n_inactive > adjusted_inactive_target:
            self.logger.info(f"  è¿‡å¤šï¼Œæ¬ é‡‡æ ·: {n_inactive} â†’ {adjusted_inactive_target}")
            df_inactive = df_inactive.sample(n=adjusted_inactive_target, random_state=42)
        elif n_inactive < adjusted_inactive_target:
            shortfall = adjusted_inactive_target - n_inactive
            status['inactive_sufficient'] = False
            status['inactive_shortfall'] = shortfall
            self.logger.warning(f"  âš ï¸  ä¸è¶³! ç¼ºå£: {shortfall}")
            self.logger.warning(f"  å°†ä½¿ç”¨æ‰€æœ‰{n_inactive}ä¸ªéæ´»æ€§æ ·æœ¬")
        else:
            self.logger.info(f"  âœ“ æ°å¥½è¾¾æ ‡: {n_inactive}")
        
        # åˆå¹¶å¹¶æ‰“ä¹±
        df_balanced = pd.concat([df_active, df_inactive], ignore_index=True)
        df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # æœ€ç»ˆç»Ÿè®¡
        final_active = (df_balanced['label'] == 1).sum()
        final_inactive = (df_balanced['label'] == 0).sum()
        final_ratio = final_inactive / max(final_active, 1)
        
        self.logger.info(f"\n{'='*70}")
        self.logger.info("æœ€ç»ˆç»“æœ")
        self.logger.info(f"{'='*70}")
        self.logger.info(f"æ´»æ€§:     {final_active:5d} (ç›®æ ‡: {self.TARGET_ACTIVE})")
        self.logger.info(f"éæ´»æ€§:   {final_inactive:5d} (ç›®æ ‡: {self.TARGET_INACTIVE})")
        self.logger.info(f"æ€»æ•°:     {len(df_balanced):5d}")
        self.logger.info(f"æ¯”ä¾‹:     1:{final_ratio:.2f} (ç›®æ ‡: 1:{self.TARGET_RATIO})")
        
        # è¾¾æ ‡è¯„ä¼°
        active_achievement = final_active / self.TARGET_ACTIVE * 100
        inactive_achievement = final_inactive / self.TARGET_INACTIVE * 100
        
        self.logger.info(f"\nè¾¾æ ‡ç‡:")
        self.logger.info(f"  æ´»æ€§:   {active_achievement:6.1f}%")
        self.logger.info(f"  éæ´»æ€§: {inactive_achievement:6.1f}%")
        
        if not status['active_sufficient'] or not status['inactive_sufficient']:
            self.logger.warning(f"\nâš ï¸  ç›®æ ‡æœªå®Œå…¨è¾¾æˆ")
            if not status['active_sufficient']:
                self.logger.warning(f"  æ´»æ€§ç¼ºå£: {status['active_shortfall']}")
                self.logger.warning(f"  å»ºè®®: æ”¾å®½IC50é˜ˆå€¼è‡³15Î¼Mæˆ–20Î¼M")
            if not status['inactive_sufficient']:
                self.logger.warning(f"  éæ´»æ€§ç¼ºå£: {status['inactive_shortfall']}")
                self.logger.warning(f"  å»ºè®®: å¢åŠ éæ´»æ€§é‡‡æ ·æ¥æº")
        else:
            self.logger.info(f"\nâœ… ç›®æ ‡å®Œç¾è¾¾æˆ!")
        
        self.logger.info(f"{'='*70}")
        
        return df_balanced, status
    
    def save_final_data(self, df: pd.DataFrame, output_path: str):
        """ä¿å­˜æœ€ç»ˆæ•°æ®"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        columns_to_save = [
            # IDå’Œç»“æ„
            'molecule_chembl_id',
            'smiles_standardized',
            'inchi_key',
            # æ´»æ€§æ•°æ®
            'standard_type',
            'standard_relation',
            'value_um',
            # æ ‡ç­¾
            'label',
            'label_rule',
            'label_confidence',
            # æè¿°ç¬¦
            'MW', 'LogP', 'TPSA', 'HBA', 'HBD',
            'RotatableBonds', 'AromaticRings', 'HeavyAtoms',
            # æº¯æº
            'data_source',
        ]
        
        # æ·»åŠ å¯é€‰åˆ—
        optional_cols = ['source_target', 'source_family', 'source_name']
        for col in optional_cols:
            if col in df.columns:
                columns_to_save.append(col)
        
        existing_cols = [col for col in columns_to_save if col in df.columns]
        df_save = df[existing_cols].copy()
        
        df_save.to_csv(output_path, index=False)
        
        self.logger.info(f"\nâœ“ æ•°æ®å·²ä¿å­˜")
        self.logger.info(f"  æ–‡ä»¶: {output_path}")
        self.logger.info(f"  è¡Œæ•°: {len(df_save)}")
        self.logger.info(f"  åˆ—æ•°: {len(existing_cols)}")
    
    def save_comprehensive_stats(self, df: pd.DataFrame, status: dict, output_path: str):
        """ä¿å­˜å®Œæ•´ç»Ÿè®¡ä¿¡æ¯"""
        import json
        
        # è¾…åŠ©å‡½æ•°ï¼šè½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_to_native(obj):
            if isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert_to_native(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_native(i) for i in obj]
            return obj
        
        stats = {
            "ç›®æ ‡è®¾å®š": {
                "target_active": int(self.TARGET_ACTIVE),
                "target_inactive": int(self.TARGET_INACTIVE),
                "target_ratio": f"1:{self.TARGET_RATIO}",
                "ic50_active_threshold_um": float(self.THRESHOLDS['IC50']['active']),
                "ec50_active_threshold_um": float(self.THRESHOLDS['EC50']['active']),
                "inactive_threshold_um": float(self.THRESHOLDS['IC50']['inactive'])
            },
            "æœ€ç»ˆç»“æœ": {
                "total_compounds": int(len(df)),
                "active_compounds": int((df['label'] == 1).sum()),
                "inactive_compounds": int((df['label'] == 0).sum()),
                "actual_ratio": f"1:{float((df['label'] == 0).sum() / max((df['label'] == 1).sum(), 1)):.2f}",
                "active_achievement_pct": float((df['label'] == 1).sum() / self.TARGET_ACTIVE * 100),
                "inactive_achievement_pct": float((df['label'] == 0).sum() / self.TARGET_INACTIVE * 100)
            },
            "è¾¾æ ‡çŠ¶æ€": convert_to_native(status),
            "IC50ç»Ÿè®¡_æ´»æ€§": {
                "count": int((df['label'] == 1).sum()),
                "mean_um": float(df[df['label'] == 1]['value_um'].mean()),
                "median_um": float(df[df['label'] == 1]['value_um'].median()),
                "min_um": float(df[df['label'] == 1]['value_um'].min()),
                "max_um": float(df[df['label'] == 1]['value_um'].max())
            },
            "IC50ç»Ÿè®¡_éæ´»æ€§": {
                "count": int((df['label'] == 0).sum()),
                "mean_um": float(df[df['label'] == 0]['value_um'].mean()),
                "median_um": float(df[df['label'] == 0]['value_um'].median()),
                "min_um": float(df[df['label'] == 0]['value_um'].min()),
                "max_um": float(df[df['label'] == 0]['value_um'].max())
            },
            "æ ‡ç­¾è§„åˆ™åˆ†å¸ƒ": {},
            "ç½®ä¿¡åº¦ç»Ÿè®¡": {
                "mean": float(df['label_confidence'].mean()),
                "std": float(df['label_confidence'].std()),
                "high_confidence_count": int((df['label_confidence'] > 0.9).sum()),
                "medium_confidence_count": int(((df['label_confidence'] >= 0.7) & (df['label_confidence'] <= 0.9)).sum()),
                "low_confidence_count": int((df['label_confidence'] < 0.7).sum())
            },
            "åˆ†å­æ€§è´¨": {
                "MW": {"mean": float(df['MW'].mean()), "std": float(df['MW'].std())},
                "LogP": {"mean": float(df['LogP'].mean()), "std": float(df['LogP'].std())},
                "TPSA": {"mean": float(df['TPSA'].mean()), "std": float(df['TPSA'].std())},
                "HBA": {"mean": float(df['HBA'].mean())},
                "HBD": {"mean": float(df['HBD'].mean())}
            }
        }
        
        # æ ‡ç­¾è§„åˆ™åˆ†å¸ƒ
        if 'label_rule' in df.columns:
            active_rules = df[df['label'] == 1]['label_rule'].value_counts().to_dict()
            inactive_rules = df[df['label'] == 0]['label_rule'].value_counts().to_dict()
            stats['æ ‡ç­¾è§„åˆ™åˆ†å¸ƒ'] = {
                "æ´»æ€§": active_rules,
                "éæ´»æ€§": inactive_rules
            }
        
        # æ•°æ®æ¥æº
        if 'data_source' in df.columns:
            stats['æ•°æ®æ¥æº'] = df['data_source'].value_counts().to_dict()
        
        output_path = Path(output_path)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"âœ“ ç»Ÿè®¡å·²ä¿å­˜: {output_path}")
    
    def run(self, input_path: str) -> str:
        """è¿è¡Œå®Œæ•´é¢„å¤„ç†æµç¨‹"""
        log_section(self.logger, "å¼€å§‹ä¸¥æ ¼é¢„å¤„ç†")
        
        # 1. åŠ è½½
        df = self.load_raw_data(input_path)
        
        # 2. å•ä½è½¬æ¢
        df = self.convert_units_strict(df)
        
        # 3. ä¸¥æ ¼æ ‡ç­¾åˆ†é…
        df = self.assign_labels_strict(df)
        
        # 4. æ ‡å‡†åŒ–å’Œè¿‡æ»¤
        df = self.standardize_and_filter(df)
        
        # 5. æ™ºèƒ½å»é‡
        df = self.intelligent_deduplication(df)
        
        # 6. ç²¾ç¡®å¹³è¡¡
        df_final, status = self.balance_to_exact_target(df)
        
        # 7. ä¿å­˜æ•°æ®
        output_dir = self.config['paths']['processed_data_dir']
        output_file = self.config['filenames']['processed_data']
        output_path = Path(output_dir) / output_file
        
        self.save_final_data(df_final, output_path)
        
        # 8. ä¿å­˜ç»Ÿè®¡
        stats_file = self.config['filenames']['data_statistics']
        stats_path = Path(output_dir) / stats_file
        self.save_comprehensive_stats(df_final, status, stats_path)
        
        # æœ€ç»ˆæ€»ç»“
        log_section(self.logger, "é¢„å¤„ç†å®Œæˆ")
        
        n_active = (df_final['label'] == 1).sum()
        n_inactive = (df_final['label'] == 0).sum()
        ratio = n_inactive / max(n_active, 1)
        
        self.logger.info(f"âœ… æœ€ç»ˆæ•°æ®é›†:")
        self.logger.info(f"  æ€»æ•°:     {len(df_final):5d}")
        self.logger.info(f"  æ´»æ€§:     {n_active:5d} (ç›®æ ‡: {self.TARGET_ACTIVE})")
        self.logger.info(f"  éæ´»æ€§:   {n_inactive:5d} (ç›®æ ‡: {self.TARGET_INACTIVE})")
        self.logger.info(f"  æ¯”ä¾‹:     1:{ratio:.2f} (ç›®æ ‡: 1:{self.TARGET_RATIO})")
        
        return str(output_path)


def main():
    """ä¸»å‡½æ•°"""
    config = load_data_config()
    
    input_dir = config['paths']['raw_data_dir']
    input_file = config['filenames']['raw_data']
    input_path = Path(input_dir) / input_file
    
    if not input_path.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {input_path}")
        print("è¯·å…ˆè¿è¡Œ: python experiments/stage0_data/01_download_strict.py")
        return
    
    preprocessor = StrictPreprocessor(config)
    
    try:
        output_path = preprocessor.run(str(input_path))
        
        print(f"\n{'='*70}")
        print("âœ… é¢„å¤„ç†å®Œæˆ")
        print(f"{'='*70}")
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"\nğŸ¯ ä¸¥æ ¼ç›®æ ‡:")
        print(f"  æ´»æ€§: 900 (IC50<10Î¼M æˆ– EC50<10Î¼M)")
        print(f"  éæ´»æ€§: 2700 (IC50>50Î¼M)")
        print(f"  æ¯”ä¾‹: 1:3")
        print(f"\nğŸ’ª æ ¸å¿ƒç‰¹ç‚¹:")
        print(f"  âœ“ ä¸¥æ ¼é˜ˆå€¼ï¼ˆIC50/EC50 < 10Î¼Mï¼‰")
        print(f"  âœ“ æ™ºèƒ½ä¸­é—´åŒºå¤„ç†")
        print(f"  âœ“ å®Œæ•´æ•°æ®æº¯æº")
        print(f"  âœ“ ç²¾ç¡®æ¯”ä¾‹æ§åˆ¶")
        print(f"\nğŸ“Š ä¸‹ä¸€æ­¥:")
        print(f"  python experiments/stage0_data/03_split_dataset.py")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
